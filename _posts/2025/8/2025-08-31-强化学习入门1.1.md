---
layout:     post
title:      强化学习入门1.1
subtitle:   马尔可夫过程
date:       2025-08-31
author:     Jiayang Hu
header-img: https://home.cnblogs.com/u/yuanchuziwen
catalog: true
tags:
    - Python
    - RL
---

原文 https://www.cnblogs.com/hitwherznchjy/p/15927190.html

博客园用markdown语法写的第一篇文章，在这里非常感谢 [小么VinVin](https://home.cnblogs.com/u/yuanchuziwen)对我的帮助。

我先在这里分享一下写强化学习的原因。大三上学期，学院开设了*智能车辆规划与决策*课程，由于学时的限制，最后决策部分的内容只讲了有限状态机和马尔可夫决策。我利用寒假这段时间，继续补充了强化学习方面的知识，大三下学期这段时间我会更新一些假期学习到的内容，从小白的视角去入门强化学习。
主要的学习资源是四个:
+ B站许志钦老师的视频（主要入门理论）https://www.bilibili.com/video/BV15a4y1j7vg?spm_id_from=333.999.0.0 
+ 书籍《强化学习入门：从原理到实践》（叶强等著，机械工业出版社）
+ github中的配套资源 https://github.com/qqiang00/Reinforce
+ 书籍《强化学习精要：核心算法与TensorFlow实现》（冯超著，中国工信出版集团）


## 1.强化学习的核心思想
一句话总结就是：**通过某种手段影响被实验体的行为**。
![](https://img2022.cnblogs.com/blog/2014676/202205/2014676-20220525225441785-1963401966.png)


我们利用上图来解释强化学习的过程。智能体**agent**（图中的大脑）在环境**environment**（图中的地球）中学习，根据环境中的状态**state**（或观测到的**observation**），执行动作**action**，并根据环境的反馈**reward**（奖励）来指导更好的动作。如何学习这个过程，如何去研究所谓的'**某种手段**'，就是强化学习的核心内容。

*注意：从环境中获取的状态，有时候叫state，有时候叫observation，这两个其实一个代表全局状态，一个代表局部观测值，我还没有学到多智能体那部分，入门就先把这两个概念划上等号。*

其他很多详细的概念暂时都没必要掌握，随着理解的深入，自然就会明白，小白也不用担心被劝退。

---

## 2.马尔可夫过程

**马尔可夫科夫性质**：在一个时序过程中，如果 $t+1$ 时的状态仅取决于 $t$ 时刻的状态$S_t$，而与$t$时刻之前的任何状态都无关，则认为$t$时刻的状态具有马尔可夫性质。

**马尔可夫过程**：具备了马尔可夫性质的随机过程称为马尔可夫过程。它是由**状态空间**和**概率空间**组成的一个二元组<S,P>。状态空间就是存储所有可能的状态*state*的集合S，而概率空间一般用状态转移矩阵P表示，如下式：
$$P=
\left[
 \begin{matrix}
   P_{11} &  \cdots & P_{1n} \\
   \vdots & & \vdots \\
   P_{n1} &  \cdots & P_{nn}
  \end{matrix} 
\right]
$$
&emsp;&emsp;其中，矩阵 P 中每一行的数据表示从某一个状态到所有 n 个状态的转移概率值。每一行的这些值加起来的和应该为 1。

由上述定义我们可以知道，马尔科夫过程中的每一个状态$St$记录了过程历史上所有相关的信息，而且一旦 St 确定了，那么历史状态信息 $S_1$ . . . $S_{t-1}$ 对于确定 $S_{t+1}$ 均不再重要，可有可无。

### 马尔可夫过程实例

![](https://img2022.cnblogs.com/blog/2014676/202205/2014676-20220525225517240-1983528052.png)


&emsp;&emsp;上图描述了一个假想的学生学习一门课程的马尔科夫过程。在这个随机过程中，学生需要顺利完成三节课并且通过最终的考试来完成这门课程的学习。当学生处在第一节课中时，会有50% 的几率拿起手机浏览社交软件信息，另有 50% 的几率完成该节课的学习进入第二节课。一
旦学生在第一节课中浏览手机社交软件信息，则有 90% 的可能性继续沉迷于浏览，而仅有 10%的几率放下手机重新听讲第一节课。学生处在第二节课的时有 80% 的几率听完第二节课顺利进入到第三节课的学习中，也有 20% 的几率因课程内容枯燥或难度较大而休息或者退出。学生在学习第三节课内容后，有 60% 的几率通过考试继而 100% 的进入休息状态，也有 40% 的几率因为过于兴奋而出去娱乐泡吧，随后可能因为忘掉了不少学到的东西而分别以 20%,40% 和 50% 的概率需要重新返回第一、二、三节课中学习。

&emsp;&emsp;上图中，我们使用内有文字的空心圆圈来描述学生可能所处的某一个状态。这些状态有：第一节课（C1）、第二节课（C2）、第三节课（C3）、泡吧中（Pub）、通过考试（Pass）、浏览手机（FB）、以及休息退出（Sleep）共 7 个状态，其中最后一个状态是终止状态，意味着学生一旦进入该状态则永久保持在该状态，或者说该状态的下一个状态将 100% 还是该状态。连接状态的箭头表示状态转移过程，箭头附近的数字表明着发生箭头所示方向状态转移的概率。
&emsp;&emsp;假设学生现处在状态“第一节课（C1）”中，我们按照马尔科夫过程给出的状态转移概率可以得到若干学生随后的状态转化序列。例如下面的这 4 个序列都是可能存在的状态转化序列：
- C1 - C2 - C3 - Pass - Sleep
- C1 - FB - FB - C1 - C2 - Sleep
- C1 - C2 - C3 - Pub - C2 - C3 - Pass - Sleep
- C1 - FB - FB - C1 - C2 - C3 - Pub - C1 - FB - FB - FB - C1 - C2 - C3 - Pub - C2 - Sleep

从符合马尔科夫过程给定的状态转移概率矩阵生成一个状态序列的过程称为**采样**（sample），基于这个概念后续会引出我们经典的*蒙特卡罗强化学习*。采样将得到一系列的状态转换过程，称为**状态序列** (episode)，在后面的很多经典项目中，都是与 episode 密切相关。如果状态序列最后一个状态属于终止状态，该状态序列称为**完整**的状态序列 (complete episode)。

---
## 3.马尔可夫奖励过程

马尔科夫过程只涉及到状态之间的转移概率，并未触及强化学习问题中伴随着状态转换的
奖励反馈。如果把奖励考虑进马尔科夫过程，则成为**马尔科夫奖励过程**（Markov reward process,**MRP**）。它是由<S, P, R, γ> 构成的一个元组，其中：
- S 是一个有限状态集
- P 是集合中状态转移概率矩阵：$P_{ss^′} = P [S_{t+1} = s^′|S_t = s]]$
- R 是一个奖励函数:$R_s = E [R_{t+1}|S_t = s]$
- γ 是一个衰减因子:γ ∈ [0, 1]

### 马尔可夫奖励过程实例

![](https://img2022.cnblogs.com/blog/2014676/202205/2014676-20220525225549580-1858921655.png)


上图与马尔可夫过程例子中的图相比，仅仅在每个状态旁增加了一个**奖励值reward**。对于我们学生来说，在上述流程中，我们希望的是在完成一个状态序列episode后，能尽可能获得多的累计奖励值。在强化学习中，我们给这个累计奖励值取一个新的名称：**收获Return**。收获的公式定义如下：
$$
G_t = R_{t+1} + γR_{t+2}+...=\sum_{k=0}^{\infty}γ^kR_{t+k+1}
$$
它是指在一个马尔科夫奖励过程中从某一个状态 $S_t$ 开始采样直到终止状态时所有
奖励的衰减之和。

下文给出了学生马尔科夫过程中四个状态序列的开始状态“第一节课”的收获值的计算，选
取 S1 =“第一节课”，γ = 0.5。
- C1 - C2 - C3 - Pass - Sleep
$G_1$ = −2 + (−2) ∗ 1/2 + (−2) ∗ 1/4 + 10 ∗ 1/8 + 0 ∗ 1/16 = −2.25
- C1 - FB - FB - C1 - C2 - Sleep
$G_1$ = −2 + (−1) ∗ 1/2 + (−1) ∗ 1/4 + (−2) ∗ 1/8 + (−2) ∗ 1/16 + 0 ∗ 1/32 = −3.125
- C1 - C2 - C3 - Pub - C2 - C3 - Pass - Sleep
$G_1$ = −2 + (−2) ∗ 1/2 + (−2) ∗ 1/4 + 1 ∗ 1/8 + (−2) ∗ 1/16 + ... = −3.41
- C1 - FB - FB - C1 - C2 - C3 - Pub - C1 - FB - FB - FB - C1 - C2 - C3 - Pub - C2 - Sleep
$G_1$ = −2 + (−1) ∗ 1/2 + (−1) ∗ 1/4 + (−2) ∗ 1/8 + (−2) ∗ 1/16 + (−2) ∗ 1/32 + ... = −3.2
### <table><tr><td bgcolor=#F48270>价值与贝尔曼方程</td></tr></table>

这一部分就开始涉及较为复杂的数学了。作为小白，虽然理解运用贝尔曼方程并不困难，但是要推导贝尔曼方程却有点麻烦。如果对下面的公式不是很理解，请去B站观看[许老师的系列课程第二节](https://www.bilibili.com/video/BV15a4y1j7vg?p=2&t=1586.7)。
**价值Value**是MRP中状态收获的期望，公式定义如下:
$$V(s) = E [G_t|S_t = s]$$
**马尔可夫奖励过程中的贝尔曼方程**如下：
$$V(s)=R_s+γ\sum_{s'}P_{ss'}V(s')$$
它提示一个状态的价值由**该状态的奖励**以及**后续状态价值**按概率分布**求和**按一定的**衰减**比例联合组成。

![](https://img2022.cnblogs.com/blog/2014676/202205/2014676-20220525225612555-847960859.png)


下面给出贝尔曼方程的详细推导：
$$\begin{aligned} 
V(s)&=E(G_t|S_t=s) \\
&\stackrel{\mathrm{①}}{=}E(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s) \\
&=E(R_{t+1}+γ(R_{t+2}+γR_{t+3}+...)|S_t=s) \\
&\stackrel{\mathrm{②}}{=}E(R_{t+1}+γG_{t+1}|S_t=s)\\
&\stackrel{\mathrm{③}}{=}E(R_{t+1}|S_t=s)+γE(G_{t+1}|S_t=s)\\
&\stackrel{\mathrm{④}}{=}R_s+γ\sum_{s'}P_{ss'}E(G_{t+1}|S_t=s, S_{t+1}=s')\\
&\stackrel{\mathrm{⑤}}{=}R_s+γ\sum_{s'}P_{ss'}E(G_{t+1}|S_{t+1}=s')\\
&\stackrel{\mathrm{⑥}}{=}R_s+γ\sum_{s'}P_{ss'}V(s')
\end{aligned}
$$
①、②：都是运用了收获的定义。
③：对于随机变量，期望的和等于和的期望。马尔可夫奖励过程是随机过程，有该性质。
> 随机过程，是依赖于参数的一组随机变量的全体，参数通常是时间。——《百度百科》

④：
- 加号左边:$R_{t+1}$ 的期望就是其自身，因为每次离开同一个状态得到的奖励都是一个固定的值。
- 加号右边的转换运用的是期望的定义。百度百科期望的定义如下：

![](https://img2022.cnblogs.com/blog/2014676/202205/2014676-20220525225635881-1897261660.png)

&emsp;&emsp;只不过我们这里的取值$X_n$也是一个期望。

⑤：这里利用的是马尔可夫性质，$S_t=s$可以不去考虑，直接去掉
⑥：这里运用的是价值的公式定义。

仍然以学生上课为例子，我们可以对第三节课的价值进行验证计算，如下：

![](https://img2022.cnblogs.com/blog/2014676/202205/2014676-20220525225654186-393287476.png)

那在实际的程序实现中，我们是怎么运用的呢？这里我们会想到矩阵。
上述 Bellman 方程可以写成如下矩阵的形式：
$$v = R + γP v$$
它表示：
$$ \left[ \begin{matrix} V(1)\\ 
\vdots\\
V(n)\end{matrix}\right]=
\left[ \begin{matrix} R_1\\ 
\vdots\\
R_n\end{matrix}\right]+γ\left[
 \begin{matrix}
   P_{11} &  \cdots & P_{1n} \\
   \vdots & & \vdots \\
   P_{n1} &  \cdots & P_{nn}
  \end{matrix} 
\right]
\left[ \begin{matrix} V(1)\\ 
\vdots\\
V(n)\end{matrix}\right]$$
该方程可以直接求解：
$$\begin{aligned}
V&=(R+γ)PV \\
(1-γP)v&=R \\
V&=(1-γP)^{-1}R
\end{aligned}$$

---
## 4.编程实践：学生马尔可夫奖励过程示例
该实例的内容已经在前文中介绍，这边仅介绍代码部分的实现过程
- 建立马尔可夫奖励过程：<S, P, R, γ>四元组
```python
import numpy as np
#num_states = 7
#{"0": "C1", "1":"C2", "2":"C3", "3":"Pass", "4":"Pub", "5":"FB", "6":"Sleep"}
i_to_n = {}
i_to_n["0"] = "C1"
i_to_n["1"] = "C2"
i_to_n["2"] = "C3"
i_to_n["3"] = "Pass"
i_to_n["4"] = "Pub"
i_to_n["5"] = "FB"
i_to_n["6"] = "Sleep"

n_to_i = {}
for i, name in zip(i_to_n.keys(), i_to_n.values()):
    n_to_i[name] = int(i)
    
#   C1   C2   C3  Pass Pub   FB  Sleep
Pss = [
   [ 0.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0 ],
   [ 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.2 ],
   [ 0.0, 0.0, 0.0, 0.6, 0.4, 0.0, 0.0 ],
   [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0 ],
   [ 0.2, 0.4, 0.4, 0.0, 0.0, 0.0, 0.0 ],
   [ 0.1, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0 ],
   [ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0 ]
]
Pss = np.array(Pss)
rewards = [-2, -2, -2, 10, 1, -1, 0]
gamma = 0.5
```
- 编写计算收获的函数
```python
def compute_return(start_index = 0, 
                   chain = None, 
                   gamma = 0.5) -> float:
    '''计算一个马尔科夫奖励过程中某状态的收获值
    Args:
        start_index 要计算的状态在链中的位置
        chain 要计算的马尔科夫过程
        gamma 衰减系数
    Returns：
        retrn 收获值
    '''
    retrn, power, gamma = 0.0, 0, gamma
    for i in range(start_index, len(chain)):
        retrn += np.power(gamma, power) * rewards[n_to_i[chain[i]]]
        power += 1
    return retrn
```
- 定义一下前例中几条以 S1 为起始状态的马尔科夫链
```python
chains =[
    ["C1", "C2", "C3", "Pass", "Sleep"],
    ["C1", "FB", "FB", "C1", "C2", "Sleep"],
    ["C1", "C2", "C3", "Pub", "C2", "C3", "Pass", "Sleep"],
    ["C1", "FB", "FB", "C1", "C2", "C3", "Pub", "C1", "FB",\
     "FB", "FB", "C1", "C2", "C3", "Pub", "C2", "Sleep"]
]
```
- 进行验证函数准确性
```python
compute_return(0, chains[3], gamma = 0.5)
# 输出结果为-3.196044921875
```
- 根据贝尔曼方程矩阵形式的公式编写计算价值的函数
```python
def compute_value(Pss, rewards, gamma = 0.05):
    '''通过求解矩阵方程的形式直接计算状态的价值
    Args：
        P 状态转移概率矩阵 shape(7, 7)
        rewards 即时奖励 list 
        gamma 衰减系数
    Return
        values 各状态的价值
    '''
    #assert(gamma >= 0 and gamma < 1.0) 
    #assert(len(P.shape) == 2 and P.shape[0] == P.shape[1])
    rewards = np.array(rewards).reshape((-1,1))
    values = np.dot(np.linalg.inv(np.eye(7,7) - gamma * Pss), rewards)
    return values
```
- 验证函数的准确性，输出结果应该和*价值与贝尔曼方程*一节中的图中的值一致
```python
values = compute_value(Pss, rewards, gamma = 0.9999)
print(values)
[[-12.51849517]
 [  1.45790178]
 [  4.3228095 ]
 [ 10.        ]
 [  0.80860462]
 [-22.49699602]
 [  0.        ]]
```

---
本次就整理到这，下次将会更新马尔可夫决策和基于模型的动态规划方法。